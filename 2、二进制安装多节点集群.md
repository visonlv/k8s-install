[TOC]

## 1、安装 VirtualBox 6.1.28

直接去virtualbox官网：https://www.virtualbox.org/下载并安装

## 2、新建虚拟机
这里使用是ubuntu server版本（ubuntu-20.04.5-live-server-amd64）进行安装，占用更小的内存。
新建虚拟机选择相关配置 2cpu 10g磁盘 桥接网络，其他使用默认即可。
按照以下方式创建三个ubuntu的虚拟机，host跟ip对应如下：
|  host   | ip  |
|  ----  | ----  |
|k8s-master1	|92.168.31.241|
|k8s-master2	|92.168.31.242|
|k8s-master3	|92.168.31.243|
|k8s-node1	|92.168.31.244|
|k8s-node2	|92.168.31.245|

### 2.1、配置固定ip
```
cat > /etc/netplan/00-installer-config.yaml << 'EOF'
network:
    ethernets:
        enp0s3:
            dhcp4: no
            dhcp6: no
            addresses:
                - 192.168.31.245/24
            routes:
                - to: default
                    via: 192.168.31.1
            nameservers:
                addresses:
                    - 8.8.8.8
    version: 2
    renderer: networkd
EOF

sudo netplan apply;
```

### 2.2、配置机器DNS
```
sed -i 's/#DNS=/DNS=8.8.8.8/g' /etc/systemd/resolved.conf
sed -i 's/#DNSStubListener=yes/DNSStubListener=no/g' /etc/systemd/resolved.conf
ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf
systemctl restart systemd-resolved
```

### 2.3、配置root账号密码
为了方便操作，统一使用root账号

```
sudo apt update;
sudo apt install vim net-tools openssh-server;
sudo passwd root;

#打开root远程登录 手动修改/etc/ssh/sshd_config
vim /etc/ssh/sshd_config;
#PermitRootLogin yes
sudo service sshd restart;
```

### 2.4、虚拟机磁盘扩容
由于开始分配的磁盘，并没有全部分配给ubuntu虚拟机，需要进行一次扩容操作
先在VirtualBox可视化操作，将新建的ubuntu虚拟机扩容到40G，然后进去ubuntu虚拟机进行磁盘的分配。

```
df -h
fdisk /dev/sda

n
4
enter
enter

t
4
8e
w
reboot

pvcreate /dev/sda4
vgextend ubuntu-vg /dev/sda4
lvextend /dev/mapper/ubuntu--vg-ubuntu--lv /dev/sda4
resize2fs /dev/ubuntu-vg/ubuntu-lv
df -h
```

## 3、配置服务器基础环境
### 3.1、设备服务器各个节点host

hostnamectl set-hostname k8s-master1;
hostnamectl set-hostname k8s-master2;
hostnamectl set-hostname k8s-master3;
hostnamectl set-hostname k8s-node1;
hostnamectl set-hostname k8s-node2;

```
cat > /etc/hosts << 'EOF'
127.0.0.1 localhost

# The following lines are desirable for IPv6 capable hosts
::1     ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters

192.168.31.241 k8s-master1
192.168.31.242 k8s-master2
192.168.31.243 k8s-master3
192.168.31.244 k8s-node1
192.168.31.245 k8s-node2
EOF
```
### 3.2、生成并同步ssh秘钥

在master1节点操作
```
ssh-keygen
for i in k8s-master{2..3}; do echo ">>> $i";ssh-copy-id $i;done
for i in k8s-node{1..2}; do echo ">>> $i";ssh-copy-id $i;done
```

### 3.3、环境变量设置

```
cat >> ~/.bashrc << 'EOF'
k8s_master1_ip=192.168.31.241
k8s_master2_ip=192.168.31.242
k8s_master3_ip=192.168.31.243
k8s_node1_ip=192.168.31.244
k8s_node2_ip=192.168.31.245
k8s_VIP="192.168.31.240"
KUBE_APISERVER="https://192.168.31.240:8443"
EOF

source ~/.bashrc
```

### 3.4、关闭防火墙 
```
ufw disable;
```

### 3.5、关闭swap
```
swapoff -a;
sed -i 's/.*swap.*/#&/g' /etc/fstab;
```

### 3.6、加载IPVS模块
```
apt-get update && apt install ipset ipvsadm socat jq -y;

cat > /etc/modules-load.d/ipvs.conf << EOF
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack
EOF

bash -x /etc/modules-load.d/ipvs.conf

cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

modprobe overlay && modprobe br_netfilter
```

### 3.7、修改tcp参数，支持内核转发
```
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system
```  	

### 3.8、配置系统文件句柄
```
cat >> /etc/security/limits.conf <<EOF
* soft nofile 65536
* hard nofile 65536
* soft nproc 65536
* hard nproc 65536
* soft  memlock  unlimited
* hard memlock  unlimited
EOF
```

## 4、部署docker环境
### 4.1、下载docker安装包
```
cd /root/
wget https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/static/stable/x86_64/docker-20.10.18.tgz --no-check-certificate
for i in k8s-master{2..3}; do echo ">>> $i";scp docker-20.10.18.tgz root@$i:/root/;done
for i in k8s-node{1..2}; do echo ">>> $i";scp docker-20.10.18.tgz root@$i:/root/;done
```

### 4.2、安装docker
```
cd /root/
tar -xf docker-20.10.18.tgz
cd /root/docker
cp ./* /usr/bin/

```

### 4.3、启动docker
```
cat > /usr/lib/systemd/system/docker.service << 'EOF'
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
EOF

systemctl enable docker --now

cat > /etc/docker/daemon.json <<EOF
{
"registry-mirrors": [
        "https://hub-mirror.c.163.com",
        "https://registry.aliyuncs.com",
        "https://registry.docker-cn.com",
        "https://docker.mirrors.ustc.edu.cn"
],
"exec-opts": ["native.cgroupdriver=systemd"],
"log-driver": "json-file",
"log-opts": {
  "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
    ]
}
EOF

systemctl daemon-reload && systemctl restart docker
```

## 5、部署Keepalived+haproxy(master节点)

如果不在云平台部署，VIP来完成多个节点的动态路由。

### 5.1、安装keepalived+haproxy
```
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "apt-get update && apt install keepalived haproxy -y";done
```

### 5.2、keepalived配置
```
cat > /etc/keepalived/keepalived.conf <<EOF 
! Configuration File for keepalived

global_defs {
    router_id LVS_k8s
}

vrrp_script check_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 3
    weight -2
    fall 10
    rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface enp0s3
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        $k8s_VIP
    }
    track_script {
        check_haproxy
    }
}
EOF
```

调整各个节点选主优先级
```
k8s-master2
sed -ri 's/MASTER/BACKUP/' /etc/keepalived/keepalived.conf
sed -ri 's/100/90/' /etc/keepalived/keepalived.conf

k8s-master3
sed -ri 's/MASTER/BACKUP/' /etc/keepalived/keepalived.conf
sed -ri 's/100/80/' /etc/keepalived/keepalived.conf
```

### 5.3、haproxy配置
```
cat > /etc/haproxy/haproxy.cfg << EOF
global
    log         127.0.0.1 local2
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon
    stats socket /var/lib/haproxy/stats
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000
frontend  kubernetes-apiserver
    mode                        tcp
    bind                        *:8443
    option                      tcplog
    default_backend             kubernetes-apiserver
listen stats
    bind            *:1080
    stats auth      admin:awesomePassword
    stats refresh   5s
    stats realm     HAProxy\ Statistics
    stats uri       /admin?stats
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server  k8s-master-01  $k8s_master1_ip:6443 check
    server  k8s-master-02  $k8s_master2_ip:6443 check
    server  k8s-master-03  $k8s_master3_ip:6443 check
EOF
```

### 5.4、haproxy监控监本
```
cat > /etc/keepalived/check_haproxy.sh<<EOF
#!/bin/sh
# HAPROXY down
A=`ps -C haproxy --no-header | wc -l`
if [ \$A -eq 0 ]
    then
systmectl start haproxy
    if [ ps -C haproxy --no-header | wc -l -eq 0 ]
        then
    killall -9 haproxy
    echo "HAPROXY down" | mail -s "haproxy"
sleep 3600
    fi 
fi
EOF
```

### 5.5、启动服务
```
# 依次启动keepalived
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "systemctl daemon-reload && systemctl restart keepalived && systemctl enable keepalived";done

# 依次启动haproxy
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "systemctl daemon-reload && systemctl restart haproxy && systemctl enable haproxy";done
```

### 5.6、查看vip目标地址
```
tcpdump -i enp0s3 -nn host 224.0.0.18
hostname -I
```

## 6、部署etcd集群(master节点)

创建相关目录
mkdir -p /etc/{kubernetes,kubernetes/ssl}
mkdir -p /etc/{etcd,etcd/ssl}
mkdir -p /opt/{etcd,kubernetes}
mkdir -p /var/log/{etcd,kubernetes}

### 6.1、准备相关二进制文件
```
# 下载etcd二进制文件
wget https://github.com/etcd-io/etcd/releases/download/v3.5.1/etcd-v3.5.1-linux-amd64.tar.gz

# 下载生成证书工具二进制文件
wget http://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /root/cfssl

wget http://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /root/cfssljson

wget http://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /root/cfssl-certinfo
```
### 6.2、安装证书工具
```
cd /root
for i in k8s-master{1..3}; do echo ">>> $i";scp  ./cfssl* root@$i:/usr/local/bin/;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "chmod +x /usr/local/bin/cfssl*";done
```

### 6.3、创建根证书(CA）
```
cat > /etc/kubernetes/ssl/ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "876000h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ],
        "expiry": "876000h"
      }
    }
  }
}
EOF

cat > /etc/kubernetes/ssl/ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "CN"
    }
  ],
  "ca": {
    "expiry": "876000h"
 }
}
EOF

# 生成CA证书和秘钥
cd /etc/kubernetes/ssl

cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```

### 6.4、创分发CA证书到所有节点
```
for i in k8s-master{1..3}; do echo ">>> $i";scp ca*.pem ca-config.json root@$i:/etc/kubernetes/ssl;done
```

### 6.5、创建etcd证书和私钥
```
# 创建etcd证书和私钥
cat > /etc/etcd/ssl/etcd-csr.json << EOF
{
    "CN": "etcd",
    "hosts": [
    "127.0.0.1",
    "$k8s_master1_ip",
    "$k8s_master2_ip",
    "$k8s_master3_ip"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "ST": "BeiJing",
            "L": "BeiJing",
            "O": "k8s",
            "OU": "CN"
        }
    ]
}
EOF

cd /etc/etcd/ssl

cfssl gencert \
-ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes etcd-csr.json | cfssljson -bare etcd


# 分发证书
for i in k8s-master{1..3}; do echo ">>> $i";scp etcd*.pem root@$i:/etc/etcd/ssl;done
```

### 6.6、etcd主配置文件
```
cat > /etc/etcd/etcd.conf << EOF
#[Member]
ETCD_NAME="etcd-1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://\$k8s_master1_ip:2380"
ETCD_LISTEN_CLIENT_URLS="https://\$k8s_master1_ip:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://\$k8s_master1_ip:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://\$k8s_master1_ip:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://$k8s_master1_ip:2380,etcd-2=https://$k8s_master2_ip:2380,etcd-3=https://$k8s_master3_ip:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF

# 修改节点1-3的etcd.conf配置文件中的节点名称
# etcd-1
for i in k8s-master1; do echo ">>> $i";ssh root@$i "sed -i 's#PEER_URLS=\"https:\/\/\$k8s_master1_ip:2380\"#PEER_URLS=\"https:\/\/$k8s_master1_ip:2380\"#g' /etc/etcd/etcd.conf";done
for i in k8s-master1; do echo ">>> $i";ssh root@$i "sed -i 's#CLIENT_URLS=\"https:\/\/\$k8s_master1_ip:2379\"#CLIENT_URLS=\"https:\/\/$k8s_master1_ip:2379\"#g' /etc/etcd/etcd.conf";done
# etcd-2
for i in k8s-master2; do echo ">>> $i";ssh root@$i "sed -ri 's#ETCD_NAME=\"etcd-1\"#ETCD_NAME=\"etcd-2\"#' /etc/etcd/etcd.conf";done
for i in k8s-master2; do echo ">>> $i";ssh root@$i "sed -i 's#PEER_URLS=\"https:\/\/\$k8s_master1_ip:2380\"#PEER_URLS=\"https:\/\/$k8s_master2_ip:2380\"#g' /etc/etcd/etcd.conf";done
for i in k8s-master2; do echo ">>> $i";ssh root@$i "sed -i 's#CLIENT_URLS=\"https:\/\/\$k8s_master1_ip:2379\"#CLIENT_URLS=\"https:\/\/$k8s_master2_ip:2379\"#g' /etc/etcd/etcd.conf";done
# etcd-3
for i in k8s-master3; do echo ">>> $i";ssh root@$i "sed -ri 's#ETCD_NAME=\"etcd-1\"#ETCD_NAME=\"etcd-3\"#' /etc/etcd/etcd.conf";done
for i in k8s-master3; do echo ">>> $i";ssh root@$i "sed -i 's#PEER_URLS=\"https:\/\/\$k8s_master1_ip:2380\"#PEER_URLS=\"https:\/\/$k8s_master3_ip:2380\"#g' /etc/etcd/etcd.conf";done
for i in k8s-master3; do echo ">>> $i";ssh root@$i "sed -i 's#CLIENT_URLS=\"https:\/\/\$k8s_master1_ip:2379\"#CLIENT_URLS=\"https:\/\/$k8s_master3_ip:2379\"#g' /etc/etcd/etcd.conf";done

```
### 6.7、安装并启动etcd
```
tar -xf etcd-v3.5.1-linux-amd64.tar.gz
for i in k8s-master{1..3}; do echo ">>> $i";scp etcd-v3.5.1-linux-amd64/{etcd,etcdctl} root@$i:/usr/local/bin/;done

cat > /usr/lib/systemd/system/etcd.service << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
EnvironmentFile=/etc/etcd/etcd.conf
WorkingDirectory=/opt/etcd
ExecStart=/usr/local/bin/etcd \\
  --cert-file=/etc/etcd/ssl/etcd.pem \\
  --key-file=/etc/etcd/ssl/etcd-key.pem \\
  --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \\
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\
  --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --enable-v2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF


for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /var/lib/etcd/default.etcd" ;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "systemctl daemon-reload" ;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "systemctl start etcd" ;done

```


## 7、部署kube-apiserver组件
### 7.1、生成kube-apiserver证书
```
cat > /etc/kubernetes/token.csv << EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF

for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/token.csv root@$i:/etc/kubernetes;done


cat > /etc/kubernetes/ssl/kube-apiserver-csr.json << EOF
{
"CN": "kubernetes",
  "hosts": [
    "127.0.0.1",
    "$k8s_master1_ip",
    "$k8s_master2_ip",
    "$k8s_master3_ip",
    "$k8s_VIP",
    "10.96.0.1",
    "kubernetes",
    "kubernetes.default",
    "kubernetes.default.svc",
    "kubernetes.default.svc.cluster",
    "kubernetes.default.svc.cluster.local"
  ],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "kubemsb",
      "OU": "CN"
    }
  ]
}
EOF

cd /etc/kubernetes/ssl/

cfssl gencert \
-ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver

for i in k8s-master{2..3}; do echo ">>> $i"; scp /etc/kubernetes/ssl/kube-apiserver*pem root@$i:/etc/kubernetes/ssl/;done
```

### 7.2、kube-apiserver主配置
```
cat > /etc/kubernetes/kube-apiserver.conf << EOF
KUBE_APISERVER_OPTS="--enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --anonymous-auth=false \\
  --bind-address=0.0.0.0 \\
  --secure-port=6443 \\
  --advertise-address=k8s_master_ip \\
  --insecure-port=0 \\
  --authorization-mode=Node,RBAC \\
  --runtime-config=api/all=true \\
  --enable-bootstrap-token-auth \\
  --service-cluster-ip-range=10.96.0.0/16 \\
  --token-auth-file=/etc/kubernetes/token.csv \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem  \\
  --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \\
  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\
  --kubelet-client-certificate=/etc/kubernetes/ssl/kube-apiserver.pem \\
  --kubelet-client-key=/etc/kubernetes/ssl/kube-apiserver-key.pem \\
  --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\
  --service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \\
  --service-account-issuer=api \\
  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\
  --etcd-certfile=/etc/etcd/ssl/etcd.pem \\
  --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\
  --etcd-servers=https://$k8s_master1_ip:2379,https://$k8s_master2_ip:2379,https://$k8s_master3_ip:2379 \\
  --enable-swagger-ui=true \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/kube-apiserver-audit.log \\
  --event-ttl=1h \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=4"
EOF

```

### 7.3、kube-apiserver服务配置
```
cat > /etc/systemd/system/kube-apiserver.service << 'EOF'
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=etcd.service
Wants=etcd.service

[Service]
EnvironmentFile=-/etc/kubernetes/kube-apiserver.conf
ExecStart=/usr/local/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF
```

### 7.4、启动kube-apiserver
```
# 下载k8s-1.23.12二进制文件
wget https://dl.k8s.io/v1.23.12/kubernetes-server-linux-amd64.tar.gz
tar -xf kubernetes-server-linux-amd64.tar.gz
#同步
cd kubernetes/server/bin/
for i in k8s-master{1..3}; do echo ">>> $i";scp kube-apiserver kube-controller-manager kube-scheduler kubectl kubeadm root@$i:/usr/local/bin/;done
for i in k8s-master{1..3}; do echo ">>> $i";scp kubelet kube-proxy root@$i:/usr/local/bin/;done

for i in k8s-master{2..3}; do echo ">>> $i"; scp /etc/systemd/system/kube-apiserver.service root@$i:/etc/systemd/system/kube-apiserver.service;done

for i in k8s-master{2..3}; do echo ">>> $i"; scp /etc/kubernetes/kube-apiserver.conf root@$i:/etc/kubernetes;done

# 1
for i in k8s-master1; do echo ">>> $i";ssh root@$i "sed -ri 's#k8s_master_ip#$k8s_master1_ip#' /etc/kubernetes/kube-apiserver.conf";done

# 2
for i in k8s-master2; do echo ">>> $i";ssh root@$i "sed -ri 's#k8s_master_ip#$k8s_master2_ip#' /etc/kubernetes/kube-apiserver.conf";done
# 3
for i in k8s-master3; do echo ">>> $i";ssh root@$i "sed -ri 's#k8s_master_ip#$k8s_master3_ip#' /etc/kubernetes/kube-apiserver.conf";done

# 启动服务
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /opt/kubernetes/kube-apiserver && systemctl daemon-reload && systemctl enable kube-apiserver --now";done
```


## 8、部署kube-controller-manager组件
### 8.1、生成kube-controller-manager证书
```
cat > /etc/kubernetes/ssl/kube-controller-manager-csr.json << EOF
{
    "CN": "system:kube-controller-manager",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "hosts": [
      "127.0.0.1",
      "$k8s_master1_ip",
      "$k8s_master2_ip",
      "$k8s_master3_ip"
    ],
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-controller-manager",
        "OU": "system"
      }
    ]
}
EOF

cd /etc/kubernetes/ssl

cfssl gencert \
-ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

for i in k8s-master{2..3}; do echo ">>> $i";scp kube-controller-manager*.pem root@$i:/etc/kubernetes/ssl;done
```

### 8.2、创建和分发 kubeconfig 文件
```
cd /etc/kubernetes/ssl
source /root/.bashrc


#1.设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=kube-controller-manager.kubeconfig

#2.设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager \
--client-certificate=kube-controller-manager.pem \
--client-key=kube-controller-manager-key.pem \
--embed-certs=true \
--kubeconfig=kube-controller-manager.kubeconfig

#设置上下文参数
kubectl config set-context system:kube-controller-manager \
--cluster=kubernetes \
--user=system:kube-controller-manager \
--kubeconfig=kube-controller-manager.kubeconfig

#设置默认上下文
kubectl config use-context system:kube-controller-manager \
--kubeconfig=kube-controller-manager.kubeconfig

for i in k8s-master{1..3}; do echo ">>> $i";scp kube-controller-manager.kubeconfig root@$i:/etc/kubernetes/;done
```

### 8.3、kube-controller-manager配置文件
```
cat > /etc/kubernetes/kube-controller-manager.conf << 'EOF'
KUBE_CONTROLLER_MANAGER_OPTS="--secure-port=10257 \
  --bind-address=127.0.0.1 \
  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \
  --service-cluster-ip-range=10.96.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --allocate-node-cidrs=true \
  --cluster-cidr=10.244.0.0/16 \
  --experimental-cluster-signing-duration=87600h \
  --root-ca-file=/etc/kubernetes/ssl/ca.pem \
  --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \
  --leader-elect=true \
  --feature-gates=RotateKubeletServerCertificate=true \
  --controllers=*,bootstrapsigner,tokencleaner \
  --horizontal-pod-autoscaler-sync-period=10s \
  --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \
  --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
  --use-service-account-credentials=true \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2"
EOF
```

### 8.4、kube-controller-manager服务配置
```
cat > /usr/lib/systemd/system/kube-controller-manager.service << 'EOF'
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-controller-manager.conf
ExecStart=/usr/local/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### 8.5、启动kube-controller-manager服务
```
for i in k8s-master{2..3}; do echo ">>> $i"; scp /usr/lib/systemd/system/kube-controller-manager.service root@$i://usr/lib/systemd/system;done

for i in k8s-master{2..3}; do echo ">>> $i"; scp /etc/kubernetes/kube-controller-manager.conf root@$i:/etc/kubernetes;done

# 启动kube-controller-manager服务
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /opt/kubernetes/kube-controller-manager && systemctl daemon-reload && systemctl start kube-controller-manager && systemctl enable kube-controller-manager";done
```


## 9、部署kube-scheduler组件
### 9.1、生成kube-scheduler证书
```
cat > /etc/kubernetes/ssl/kube-scheduler-csr.json <<EOF
{
    "CN": "system:kube-scheduler",
    "hosts": [
      "127.0.0.1",
      "$k8s_master1_ip",
      "$k8s_master2_ip",
      "$k8s_master3_ip"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
      {
        "C": "CN",
        "ST": "Beijing",
        "L": "Beijing",
        "O": "system:kube-scheduler",
        "OU": "system"
      }
    ]
}
EOF

cd /etc/kubernetes/ssl

cfssl gencert \
-ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler

for i in k8s-master{2..3}; do echo ">>> $i";scp kube-scheduler*pem root@$i:/etc/kubernetes/ssl;done
```

### 9.2、创建和分发 kubeconfig 文件
```
# "设置集群参数"
 kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=kube-scheduler.kubeconfig

# "设置客户端认证参数"
 kubectl config set-credentials system:kube-scheduler \
--client-certificate=kube-scheduler.pem \
--client-key=kube-scheduler-key.pem \
--embed-certs=true \
--kubeconfig=kube-scheduler.kubeconfig

# "设置上下文参数"
 kubectl config set-context system:kube-scheduler \
--cluster=kubernetes \
--user=system:kube-scheduler \
--kubeconfig=kube-scheduler.kubeconfig

# "设置默认上下文"
 kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig
 
# 分发 kubeconfig 到所有 master 节点
for i in k8s-master{1..3}; do echo ">>> $i";scp kube-scheduler.kubeconfig root@$i:/etc/kubernetes/;done
```

### 9.3、kube-scheduler主配置
```
cat > /etc/kubernetes/kube-scheduler.conf << 'EOF'
KUBE_SCHEDULER_OPTS="--address=127.0.0.1 \
--kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \
--leader-elect=true \
--alsologtostderr=true \
--logtostderr=false \
--log-dir=/var/log/kubernetes \
--v=2"
EOF
```

### 9.4、kube-scheduler服务配置
```
cat > /usr/lib/systemd/system/kube-scheduler.service << 'EOF'
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/etc/kubernetes/kube-scheduler.conf
ExecStart=/usr/local/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### 9.5、启动kube-scheduler
```
for i in k8s-master{2..3}; do echo ">>> $i";scp /usr/lib/systemd/system/kube-scheduler.service root@$i:/usr/lib/systemd/system;done

for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/kube-scheduler.conf root@$i:/etc/kubernetes;done

# 启动kube-scheduler服务
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /opt/kubernetes/kube-scheduler && mkdir -p /var/log/kubernetes/kube-schedul && systemctl daemon-reload && systemctl start kube-scheduler && systemctl enable kube-scheduler";done
```

## 10、部署kubectl组件
### 10.1、生成kubectl证书
```
cat > /etc/kubernetes/ssl/admin-csr.json << EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",             
      "OU": "system"
    }
  ]
}
EOF

cd /etc/kubernetes/ssl/

cfssl gencert \
-ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes admin-csr.json | cfssljson -bare admin
```

### 10.2、创建~/.kube/config文件
```
# 1.设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=/root/.kube/config

# 2.设置客户端认证参数
kubectl config set-credentials admin \
--client-certificate=/etc/kubernetes/ssl/admin.pem \
--client-key=/etc/kubernetes/ssl/admin-key.pem \
--embed-certs=true \
--kubeconfig=/root/.kube/config

# 3.设置上下文参数
kubectl config set-context kubernetes \
--cluster=kubernetes \
--user=admin \
--kubeconfig=/root/.kube/config

# 4.设置默认上下文
kubectl config use-context kubernetes \
--kubeconfig=/root/.kube/config

# 分发
for i in k8s-master{2..3}; do echo ">>> $i";ssh root@$i "mkdir -p ~/.kube" && scp /root/.kube/config root@$i:~/.kube/config;done
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes --kubeconfig=/root/.kube/config
```

## 11、部署kubelet组件
### 11.1、创建和分发 kubeconfig 文件
```
# token 载入环境变量
BOOTSTRAP_TOKEN=$(awk -F "," '{print $1}' /etc/kubernetes/token.csv)

# 2.设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig

# 3.设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
--token=${BOOTSTRAP_TOKEN} \
--kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig

# 4.设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=kubelet-bootstrap \
--kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig

# 5.设置默认上下文
kubectl config use-context default --kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig
kubectl create clusterrolebinding cluster-system-anonymous --clusterrole=cluster-admin --user=kubelet-bootstrap
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap --user=/etc/kubernetes/kubelet-bootstrap.kubeconfig

# 分发
for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/kubelet-bootstrap.kubeconfig root@$i:/etc/kubernetes/;done
```

### 11.2、kubelet主配置文件
```
cat > /etc/kubernetes/kubelet.json <<EOF
{
  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
  "authentication": {
    "x509": {
      "clientCAFile": "/etc/kubernetes/ssl/ca.pem"
    },
    "webhook": {
      "enabled": true,
      "cacheTTL": "2m0s"
    },
    "anonymous": {
      "enabled": false
    }
  },
  "authorization": {
    "mode": "Webhook",
    "webhook": {
      "cacheAuthorizedTTL": "5m0s",
      "cacheUnauthorizedTTL": "30s"
    }
  },
  "address": "0.0.0.0",
  "port": 10250,
  "readOnlyPort": 10255,
  "cgroupDriver": "systemd",
  "hairpinMode": "promiscuous-bridge",
  "serializeImagePulls": false,
  "clusterDomain": "cluster.local.",
  "clusterDNS": ["10.96.0.10"]
}
EOF

for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/kubelet.json root@$i:/etc/kubernetes/;done
```

### 11.3、kubelet服务配置
```
cat > /usr/lib/systemd/system/kubelet.service <<'EOF'
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/usr/local/bin/kubelet \
  --bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/ssl \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --config=/etc/kubernetes/kubelet.json \
  --cni-bin-dir=/opt/cni/bin \
  --cni-conf-dir=/etc/cni/net.d \
  --network-plugin=cni \
  --rotate-certificates \
  --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.6 \
  --root-dir=/etc/cni/net.d \
  --alsologtostderr=true \
  --logtostderr=false \
  --log-dir=/var/log/kubernetes \
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

### 11.4、启动kubelet
```
for i in k8s-master{2..3}; do echo ">>> $i";scp /usr/lib/systemd/system/kubelet.service root@$i:/usr/lib/systemd/system;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i docker pull registry.aliyuncs.com/google_containers/pause:3.6;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /var/log/kubernetes && mkdir /var/lib/kubelet && systemctl daemon-reload && systemctl start kubelet && systemctl enable kubelet";done
```

## 12、部署kube-proxy组件
### 12.1、创建跟分发证书
```
cat > /etc/kubernetes/ssl/kube-proxy-csr.json <<EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "kubemsb",
      "OU": "CN"
    }
  ]
}
EOF

cd /etc/kubernetes/ssl

cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \
-ca-key=/etc/kubernetes/ssl/ca-key.pem \
-config=/etc/kubernetes/ssl/ca-config.json \
-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

for i in k8s-master{2..3}; do echo ">>> $i";scp kube-proxy*pem root@$i:/etc/kubernetes/ssl;done
```

### 12.2、创建和分发 kubeconfig 文件
```
# 设置集群参数
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kube-proxy \
--client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
--client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
--embed-certs=true \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

# 设置上下文参数
kubectl config set-context default \
--cluster=kubernetes \
--user=kube-proxy \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig

for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/kube-proxy.kubeconfig root@$i:/etc/kubernetes;done
```
### 12.3、kube-proxy主配置文件
```
cat > /etc/kubernetes/kube-proxy-config.yaml <<EOF
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
clientConnection:
  kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig
clusterCIDR: 10.244.0.0/16
healthzBindAddress: 127.0.0.1:10256
kind: KubeProxyConfiguration
metricsBindAddress: 127.0.0.1:10249
mode: "ipvs"
EOF

```

### 12.4、kube-proxy服务配置
```
cat > /usr/lib/systemd/system/kube-proxy.service << EOF
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/etc/kubernetes/kube-proxy-config.yaml \\
  --alsologtostderr=true \\
  --logtostderr=false \\
  --log-dir=/var/log/kubernetes \\
  --v=2

Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

```

### 12.5、启动kube-proxy
```
for i in k8s-master{2..3}; do echo ">>> $i";scp /etc/kubernetes/kube-proxy-config.yaml root@$i:/etc/kubernetes;done
for i in k8s-master{2..3}; do echo ">>> $i";scp /usr/lib/systemd/system/kube-proxy.service root@$i:/usr/lib/systemd/system;done
for i in k8s-master{1..3}; do echo ">>> $i";ssh root@$i "mkdir -p /var/lib/kube-proxy && systemctl daemon-reload && systemctl start kube-proxy && systemctl enable kube-proxy";done
```


## 13 网络组件calico
```
wget https://raw.fastgit.org/projectcalico/calico/v3.24.1/manifests/calico.yaml

1 修改CIDR(大概4546行)
- name: CALICO_IPV4POOL_CIDR
  value: "10.244.0.0/16"

2 指定网卡
# Cluster type to identify the deployment type
  - name: CLUSTER_TYPE
  value: "k8s,bgp"
# 下面添加
  - name: IP_AUTODETECTION_METHOD
    value: "interface=ens33"
    # ens33为本地网卡名字（自己机器啥网卡就改啥）
  
kubectl apply -f calico.yaml
```

## 14 dns组件coredns
```
wget https://github.com/coredns/deployment/archive/refs/tags/coredns-1.14.0.tar.gz

wget https://download.fastgit.org/coredns/deployment/archive/refs/tags/coredns-1.14.0.tar.gz

tar xf coredns-1.14.0.tar.gz && cd deployment-coredns-1.14.0/kubernetes


coredns:v1.8.0替换成
registry.aliyuncs.com/google_containers/coredns:v1.8.0

./deploy.sh -i "10.96.0.10" -d "cluster.local" | kubectl apply -f -

kubectl run -it --rm dns-test --image=busybox:1.28.4 sh

额外配置hosts
Corefile: |
.:53 {
    errors
    health {
      lameduck 5s
    }
    ready
    kubernetes CLUSTER_DOMAIN REVERSE_CIDRS {
      fallthrough in-addr.arpa ip6.arpa
    }

    hosts {
      192.168.31.241     k8s-master1
      192.168.31.242     k8s-master2
      192.168.31.243     k8s-master3
      192.168.31.244     k8s-node1
      192.168.31.245     k8s-node2
    }

    prometheus :9153
    forward . UPSTREAMNAMESERVER {
      max_concurrent 1000
    }
    cache 30
    loop
    reload
    loadbalance
}STUBDOMAINS
```

## 15 测试
```
cat > nginx.yml << 'EOF'
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30080
  type: NodePort
EOF

kubectl apply -f nginx.yml
```